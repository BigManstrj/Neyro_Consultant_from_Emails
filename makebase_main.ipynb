{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' модуль загрузки библиотек и преподготовки базы '''\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import importlib\n",
    "import mdTOdocx\n",
    "import requests\n",
    "import os\n",
    "\n",
    "''' модуль загрузки данных из файла и очистки от личной информации '''\n",
    "# функця считывания ключа openAI из .env\n",
    "def get_key_ОpenAI():\n",
    "    # подгружаем переменные окружения\n",
    "    load_dotenv()\n",
    "    # токен бота\n",
    "    openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "def remove_personal_data(text):\n",
    "    # Удаление email адресов\n",
    "    text = re.sub(r'\\S+@\\S+', '<email>', text)\n",
    "    # Удаление имен и фамилий (примерный паттерн)\n",
    "    text = re.sub(r'\\b[A-Z][a-z]+ [A-Z][a-z]+\\b', '<name>', text)\n",
    "    # Удаление номеров телефонов\n",
    "    text = re.sub(r'\\+?\\d[\\d -]{8,}\\d', '<phone>', text)\n",
    "    # Удаление номеров билетов и бронирования\n",
    "    text = re.sub(r'\\b\\d{6,10}\\b', '<ticket_number>', text)\n",
    "    # Удаление номеров паспортов\n",
    "    text = re.sub(r'\\b[A-Z0-9]{8,10}\\b', '<passport_number>', text)\n",
    "    return text\n",
    "\n",
    "def make_dirs():\n",
    "      \"\"\" модуль создания необходимых каталогов \"\"\"\n",
    "      base_dir = 'base/'\n",
    "      # проверяем наличие каталога 'base' при необходимости создаем\n",
    "      if not os.path.exists(base_dir):\n",
    "            # Если папка не существует, создаем её\n",
    "                  os.makedirs(base_dir)\n",
    "\n",
    "      # проверяем наличие подкаталогов 'input','output','structured'. При необходимости создаем\n",
    "      base_dirs = ['input','output','structured', 'promts'] # папки которые должны быть созданы внутри каталога base\n",
    "      for d in base_dirs:\n",
    "            if not os.path.exists(f'{base_dir}{d}'):\n",
    "            # Если папка не существует, создаем её\n",
    "                  os.makedirs(f'{base_dir}{d}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' модуль функций. обработки писем - извлечение вопрос/ответ и категоризация '''\n",
    "def call_gpt_api(prompt):\n",
    "    ''' функция обращения к chatGPT'''\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.1\n",
    "            )\n",
    "            return response.choices[0].message['content']\n",
    "        except openai.error.RateLimitError:\n",
    "            print(\"Rate limit exceeded. Waiting for 60 seconds before retrying...\")\n",
    "            time.sleep(60)\n",
    "        except openai.error.OpenAIError as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "def extract_segments(cleaned_text):\n",
    "    ''' функция извлечения писем из обработанного файла txt '''\n",
    "\n",
    "    segments = cleaned_text.split(\"## Текст письма №\")\n",
    "    return segments\n",
    "\n",
    "def process_segment(segment):\n",
    "    ''' функция первой обработки письма - извлечение вопрос/ответ '''\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Act as a specialist in extracting entities from text and preserving confidential information. \n",
    "    Your task is to extract the question and answer from the document, and then check the question and answer for confidential information (surname, name, patronymic, passport number, ticket number, reservation number, phone number). If such information is present, delete it. If the question and answer do not contain useful information for passenger support service - delete these messages. Summarize the question and answer, leaving only the essence. Do not make up anything on your own. Base your question only on the information provided to you. \n",
    "    If there is no question or answer, delete these messages. \n",
    "    If you need to provide support contacts in your reply, please specify e-mail: MEGACOMPANY@MEGACOMPANY.com tel: +0-00-000-000.\n",
    "    Write in Russian! If the question or answer is not written in Russian, please translate it into Russian!\n",
    "        \n",
    "    Message: {segment}\n",
    "    \n",
    "    JSON output format:\n",
    "    {{\n",
    "        \"question\": \"\",\n",
    "        \"answer\": \"\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    result = call_gpt_api(prompt)\n",
    "    return result\n",
    "\n",
    "def extract_qa_pairs(segments):\n",
    "    ''' функция первой обработки всех писем - извлечение вопрос/ответ '''\n",
    "\n",
    "    aq_dict = []\n",
    "    for segment in segments:\n",
    "        processed = process_segment(segment)\n",
    "        \n",
    "        # print(f'\\nОТрабатываем сегмент:\\n{segment}\\n')\n",
    "        # print(f'Вывод функции process_segment\\n{processed}\\n')\n",
    "        \n",
    "        if processed != '{question:, answer:}':\n",
    "            try:\n",
    "                processed_dict = json.loads(processed) \n",
    "                if processed_dict['question'] and processed_dict['answer']:\n",
    "\n",
    "                    # print(f'Добавляем в словарь:\\n{processed_dict}')\n",
    "\n",
    "                    aq_dict.append(processed_dict)\n",
    "                \n",
    "            except:\n",
    "                print(f'\\nисключен  результат process_segment\\n{processed}\\n')\n",
    "    return aq_dict\n",
    "\n",
    "def remove_duplicate_questions_list(qa_pairs):\n",
    "    ''' функция удаления дубликатов из пар вопрос/ответ'''\n",
    "\n",
    "    questions = [pair['question'] for pair in qa_pairs]\n",
    "    vectorizer = TfidfVectorizer().fit_transform(questions)\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    to_remove = set()\n",
    "    for i in range(len(cosine_matrix)):\n",
    "        for j in range(i+1, len(cosine_matrix)):\n",
    "            if cosine_matrix[i][j] > 0.85:  # Порог схожести\n",
    "                to_remove.add(j)\n",
    "    \n",
    "    unique_qa_pairs = [qa_pairs[i] for i in range(len(qa_pairs)) if i not in to_remove]\n",
    "    return unique_qa_pairs\n",
    "\n",
    "def determine_categories(questions:list):\n",
    "    ''' функция определния категорий из конечного списка вопросов '''\n",
    "\n",
    "    all_questions = \", \".join(questions)\n",
    "    # all_questions=questions\n",
    "    prompt = f\"Answer only in Russian. Based on the following questions, identify no more than 20 main categories and 20 subcategories:\\n{all_questions}.\"\n",
    "    categories = call_gpt_api(prompt)\n",
    "    return categories\n",
    "\n",
    "def classify_question(question, categories):\n",
    "    ''' функция определения категории одного вопроса '''\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    К какой из представленных  категорий относится следующий вопрос? \n",
    "    \\nВопрос:{question}\\nКатегории:{categories}.\n",
    "    Выведи ответ в формате json:\n",
    "    {{\"category\": \"\"}}\n",
    "    \"\"\"\n",
    "    category = call_gpt_api(prompt)\n",
    "    if not category:\n",
    "        print(\"Received empty response from API.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Очистим строку от лишних символов\n",
    "        clean_category = category.strip().replace('```json', '').replace('```', '')\n",
    "\n",
    "        # Используем регулярное выражение для извлечения текста между фигурными скобками\n",
    "        match = re.search(r'(\\{.*?\\})', clean_category)\n",
    "\n",
    "        if match:\n",
    "            clean_category = match.group(1)\n",
    "            category_json = json.loads(clean_category)\n",
    "            return category_json\n",
    "        else:\n",
    "            # print(\"Категории не определны в нужном формате\")\n",
    "            category_json = json.loads(clean_category)\n",
    "            return category_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON response: {e}\")\n",
    "        print(f\"Response was: {category}\")\n",
    "        return None\n",
    "\n",
    "def create_knowledge_base(qa_pairs_json:json, categories):\n",
    "    ''' функция создания базы знаний из окончательно обработанных пар вопрос/ответ '''\n",
    "\n",
    "    sections = {category: [] for category in categories.split(\"\\n\") if category}\n",
    "    sections[\"Прочие\"] = []  # Обязательно добавляем категорию \"Прочие\"\n",
    "\n",
    "    # Получение всех подкатегорий\n",
    "    subcategories = \"\\n\".join([category for category in sections.keys() if category and category.strip().startswith('-')])\n",
    "\n",
    "    # print(subcategories) #!!!!!!!!!!!\n",
    "\n",
    "    for pair in json.loads(qa_pairs_json):\n",
    "        question, answer = pair['question'], pair['answer']\n",
    "        category = classify_question(question, subcategories)\n",
    "        \n",
    "\n",
    "        # print(f'\\nДля вопроса \\n{question}\\nопределена категория:{category}\\n')\n",
    "\n",
    "        added = False  # Флаг для отслеживания, была ли категория добавлена\n",
    "        if category and 'category' in category:\n",
    "            for category_key in sections.keys():\n",
    "                if category['category'] in category_key:\n",
    "                    sections[category_key].append(pair)\n",
    "                    added = True\n",
    "                    break\n",
    "\n",
    "        if not added:  # Если ни одна категория не подошла, добавляем в \"Другое\"\n",
    "            sections[\"Прочие\"].append(pair)\n",
    "            \n",
    "    knowledge_base = \"\"\n",
    "    for section, pairs in sections.items():\n",
    "\n",
    "        # print(f'\\nsection:\\n {section}\\n')\n",
    "\n",
    "        # корректно разобьем маркдуаун  на категории и подкатегории \n",
    "        if section.strip().startswith('-'):          \n",
    "            knowledge_base += f\"## {section}\\n\"\n",
    "        else:\n",
    "            knowledge_base += f\"# {section}\\n\"\n",
    "\n",
    "        for pair in pairs:\n",
    "            knowledge_base += f\"### Вопрос: {pair['question']}\\n- Ответ: {pair['answer']}\\n\"\n",
    "\n",
    "    return knowledge_base\n",
    "\n",
    "def markdown_to_dataframe(md_file_path):\n",
    "    ''' функция конвертации md файла базы знаний в df pandas '''\n",
    "\n",
    "    with open(md_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data = {'Category': [], 'Subcategory': [], 'Question': [], 'Answer': []}\n",
    "    current_category = None\n",
    "    current_subcategory = None\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('# '):\n",
    "            current_category = line[2:].strip()\n",
    "            current_subcategory = None\n",
    "        elif line.startswith('## '):\n",
    "            current_subcategory = line[3:].strip()\n",
    "        elif line.startswith('### Вопрос: '):\n",
    "            # question = line[10:].strip().rstrip('**')\n",
    "            question = line[11:].strip().rstrip('**')\n",
    "            answer_index = i + 1\n",
    "            if answer_index < len(lines) and lines[answer_index].startswith('- Ответ:'):\n",
    "                answer = lines[answer_index][9:].strip()\n",
    "                data['Category'].append(current_category)\n",
    "                data['Subcategory'].append(current_subcategory)\n",
    "                data['Question'].append(question)\n",
    "                data['Answer'].append(answer)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def dataframe_to_markdown(df, md_file_path):\n",
    "    ''' функция обратной конвертации файла базы знаний из df pandas в md  '''\n",
    "\n",
    "    with open(md_file_path, 'w', encoding='utf-8') as file:\n",
    "        for category in df['Category'].unique():\n",
    "            file.write(f'# {category}\\n')\n",
    "            category_df = df[df['Category'] == category]\n",
    "            for subcategory in category_df['Subcategory'].unique():\n",
    "                if subcategory and subcategory != \"None\":\n",
    "                    file.write(f'## {subcategory}\\n')\n",
    "                else:\n",
    "                    pass\n",
    "                    # file.write(f'## Без подкатегории\\n')  # Добавлено для обработки None\n",
    "                subcategory_df = category_df[category_df['Subcategory'] == subcategory]\n",
    "                for _, row in subcategory_df.iterrows():\n",
    "                    question = row['Question']\n",
    "                    answer = row['Answer']\n",
    "                    file.write(f\"### Вопрос: {question}\\n- Ответ: {answer}\\n\")\n",
    "\n",
    "def remove_duplicate_questions(df):\n",
    "    ''' Повторная проверка и удаление одинаковых по смыслу вопросов (из df pandas) '''\n",
    "\n",
    "    vectorizer = TfidfVectorizer().fit_transform(df['Question'])\n",
    "    vectors = vectorizer.toarray()\n",
    "    cosine_matrix = cosine_similarity(vectors)\n",
    "    \n",
    "    to_remove = []\n",
    "    for i in range(len(cosine_matrix)):\n",
    "        for j in range(i+1, len(cosine_matrix)):\n",
    "            if cosine_matrix[i][j] > 0.5:  # Порог схожести\n",
    "                to_remove.append(j)\n",
    "    \n",
    "    df = df.drop(to_remove)\n",
    "    return df\n",
    "\n",
    "def clean_with_chatgpt(qa_pairs):\n",
    "    ''' вторая обработка базы знаний через chatGPT '''\n",
    "\n",
    "    cleaned_pairs = []\n",
    "    for pair in qa_pairs:\n",
    "        question = pair.get(\"question\", \"\").strip()\n",
    "        answer = pair.get(\"answer\", \"\").strip()\n",
    "\n",
    "        # Prompt for ChatGPT to clean the data\n",
    "        prompt = f\"\"\"\n",
    "        Вопрос: {question}\n",
    "        Ответ: {answer}\n",
    "        ---\n",
    "        Удали все персональные данные и удали пары, где ответ не является корректным или не содержит полезной информации. Переформулируй ответы и вопросы, если это необходимо.\n",
    "        Вот так должны выглядеть идеальные пары вопрос-ответ:\n",
    "        \n",
    "        Где я могу посмотреть самые дешевые варианты перелета?\n",
    "        При поиске минимальные тарифы будут первыми в предложенном списке.\n",
    "\n",
    "        Как получить маршрут-квитанцию на удобном мне языке?\n",
    "        Маршрут-квитанция предоставляется на трех языках (казахском, русском и английском).\n",
    "\n",
    "        Как правильно указать фамилию и имя латиницей при покупке билета?\n",
    "        Правильная транслитерация фамилии и имени указана в вашем паспорте/удостоверении личности.\n",
    "\n",
    "        Как я могу оформить билет для младенца до 2-х лет?\n",
    "        Если билет был куплен на официальном сайте авиакомпании www.MEGACOMPANY.com, вам необходимо отправить запрос в Авиакомпанию с документами младенца, прикрепив билет взрослого. Если же билет был приобретен на стороннем сайте или кассе, необходимо обратиться по месту приобретения билета.\n",
    "        \"\"\"\n",
    "\n",
    "        cleaned_text = call_gpt_api(prompt)\n",
    "\n",
    "        # cleaned_text = response.choices[0].text.strip()\n",
    "        if cleaned_text:\n",
    "            cleaned_pair = cleaned_text.split('\\n')\n",
    "            if len(cleaned_pair) == 2:\n",
    "                cleaned_question = cleaned_pair[0].replace(\"Вопрос: \", \"\").strip()\n",
    "                cleaned_answer = cleaned_pair[1].replace(\"Ответ: \", \"\").strip()\n",
    "                if cleaned_question and cleaned_answer:\n",
    "                    cleaned_pairs.append({\n",
    "                        \"question\": cleaned_question,\n",
    "                        \"answer\": cleaned_answer\n",
    "                    })\n",
    "    return cleaned_pairs\n",
    "\n",
    "# Function to clean question-answer pairs using ChatGPT\n",
    "def clean_with_chatgpt_else(qa_pairs):\n",
    "    ''' третья обработка базы знаний через chatGPT '''\n",
    "\n",
    "    cleaned_pairs = []\n",
    "    for pair in qa_pairs:\n",
    "        question = pair.get(\"question\", \"\").strip()\n",
    "        answer = pair.get(\"answer\", \"\").strip()\n",
    "\n",
    "        # Prompt for ChatGPT to clean the data\n",
    "        prompt = f\"\"\"\n",
    "        Преобразуй вопросы пассажиров и ответы службы клиентской поддержки авиакомпании MEGACOMPANY, которые я напишу  следующим образом:\n",
    "        - измени ответ так, чтобы он содержал в начале саму суть вопроса, в виде оборота для ответа \n",
    "        Пример: \n",
    "        **Вопрос: как мне выполнить большую задачу?**\n",
    "        Правильный Ответ: Чтобы выполнить большую задачу необходимо разбить ее на подзадачи и решить их отдельно.\n",
    "        Неправильный Ответ: Необходимо разбить ее на подзадачи и решить их отдельно.\n",
    "\n",
    "        - если в вопросе есть просьба, то преобразуй ее в вопрос (Например \"Помогите сделать\" -> \"Как мне сделать?\")\n",
    "        - оставь в вопросе и ответе только самое  важное\n",
    "        - убери из ответов и вопросов детали  конкретного случая (например конкретные детали заказа: города и аэропорты, даты, время вылета, имена пассажиров). Если данная ситуация не может повториться у другого пассажира и вопрос не является типовым  - напиши \"Соединяю с оператором\"\n",
    "        - Обобщи ответ или вопрос если упоминается  конкретная организация. Например вместо \"банк Сбер\" необходимо указать \"банк\"\n",
    "        - учти, что все билеты куплены в авиакомпании MEGACOMPANY\n",
    "        - не выводи контактные данные авиакомпании\n",
    "        - убери все рекомендации связаться с авиакомпанией\n",
    "\n",
    "        Если вопрос или ответ бессмысленные, напиши \"Игнорирую\"\n",
    "\n",
    "        Пример:\n",
    "        Вопрос: **Как я могу изменить дату бронирования билетов в Самару для семьи из трех человек? Купил в агентстве Каспи**\n",
    "        - неправильный Ответ: Для изменения даты бронирования билетов для семьи из трех человек в Самару вам необходимо связаться с агентством Каспи.\n",
    "        - Правильный Ответ: Для изменения даты бронирования билетов  вам необходимо связаться с агентством, в котором был куплен билет.\n",
    "\n",
    "        Пример:\n",
    "        Неправильный Вопрос: **Как исправить имя в билете, где указано неправильное имя ILIA вместо ИЛИЯ?**\n",
    "        Правильный вопрос: **Как исправить имя в билете?**\n",
    "\n",
    "        Вопрос: {question}\n",
    "        Ответ: {answer}\n",
    "        \"\"\"\n",
    "\n",
    "        cleaned_text = call_gpt_api(prompt)\n",
    "\n",
    "        # cleaned_text = response.choices[0].text.strip()\n",
    "        if cleaned_text:\n",
    "            cleaned_pair = cleaned_text.split('\\n')\n",
    "            if len(cleaned_pair) == 2:\n",
    "                cleaned_question = cleaned_pair[0].replace(\"Вопрос: \", \"\").strip()\n",
    "                cleaned_answer = cleaned_pair[1].replace(\"Ответ: \", \"\").strip()\n",
    "                if cleaned_question and cleaned_answer:\n",
    "                    cleaned_pairs.append({\n",
    "                        \"question\": cleaned_question,\n",
    "                        \"answer\": cleaned_answer\n",
    "                    })\n",
    "    return cleaned_pairs\n",
    "\n",
    "def read_json(file_path):\n",
    "    ''' Функция для чтения JSON из файла '''\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def write_json(data, file_path):\n",
    "    ''' Функция для записи JSON в файл '''\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_document_text(url: str, docx_file:str) -> str:\n",
    "    ''' функция для загрузки документа docx по ссылке из гугл драйв '''\n",
    "\n",
    "    # Extract the document ID from the URL\n",
    "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
    "    if match_ is None:\n",
    "        raise ValueError('Invalid Google Docs URL')\n",
    "    doc_id = match_.group(1)\n",
    "\n",
    "    # print(doc_id)    \n",
    "    # print(f'https://docs.google.com/document/d/{doc_id}/export?format=docx')\n",
    "    \n",
    "    # Download the document as plain text\n",
    "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=docx')\n",
    "    response.raise_for_status()\n",
    "    text = response.content\n",
    "\n",
    "    with open(docx_file, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    return text\n",
    "\n",
    "def md_to_excel(md_file_path:str, excel_file:str):\n",
    "    ''' конвертация md в excel '''\n",
    "    # md_file_path = f'{base_dir}/output/base_final.md'\n",
    "    df = markdown_to_dataframe(md_file_path)\n",
    "\n",
    "    # сохраним в excel для проверки\n",
    "    df.to_excel(excel_file)   # !!!! раскоментить перед правкой\n",
    "    return\n",
    "\n",
    "''' правка excel '''\n",
    "\n",
    "def excel_to_md(excel_file:str, md_file_path:str):\n",
    "    ''' конвертация excel в md '''\n",
    "   \n",
    "    # загрузим из excel\n",
    "    df = pd.read_excel(excel_file)  # !!!! раскоментить после правки\n",
    "\n",
    "    # сохрание базы в md файл\n",
    "    dataframe_to_markdown(df, md_file_path)\n",
    "    return\n",
    "\n",
    "def google_docx_to_md(docx_url:str, docx_file:str):\n",
    "    ''' функция загрузки базы из google docx, конвертация в md и сохранения на диске '''\n",
    "\n",
    "\n",
    "    # docx_file = 'base/output/base_final.docx'\n",
    "    # docx_url = 'https://docs.google.com/document/d/1idE1tls-N.............'\n",
    "    load_document_text(docx_url, docx_file)\n",
    "\n",
    "    # конвертация в md для базы знаний\n",
    "    mdTOdocx.convert_docx_to_md(docx_file, 'base/output/base_final.md')\n",
    "\n",
    "    return\n",
    "\n",
    "def md_to_dict_list(md_file_path):\n",
    "    ''' функция перевода базы из md в список словарей для последущей обработки '''\n",
    "\n",
    "    # перевод базы в df  и удаление дубликатов\n",
    "    md_file_path = 'base/output/base_final.md'\n",
    "    df = markdown_to_dataframe(md_file_path)\n",
    "    \n",
    "    def create_json(row):\n",
    "        return {'question':row['Question'],'answer':row['Answer']}\n",
    "\n",
    "    clean_unique_qa_pairs = list(df.apply(create_json, axis=1))\n",
    "\n",
    "    # возращаем список словарей если понадобиться\n",
    "    return clean_unique_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_round_make_base(cleaned_data:str) -> list:\n",
    "    ''' модуль создания чернового варианта базы знаний из очищенной переписки '''\n",
    "    # # Загрузка очищенных данных из файла base base/output/emails.txt\n",
    "    # with open(output_filename, 'r', encoding='utf-8') as file:\n",
    "    #     cleaned_data = file.read()\n",
    "\n",
    "    # Извлечение сегментов сообщений \n",
    "    segments = extract_segments(cleaned_data)  #  !!!! Ограничиваем первые 10000 символов для тестирования\n",
    "\n",
    "    # Извлечение вопросов и ответов с помощью chatGPT\n",
    "    qa_pairs = extract_qa_pairs(segments)\n",
    "\n",
    "    # Удаление дублирующихся вопросов\n",
    "    unique_qa_pairs = remove_duplicate_questions_list(qa_pairs)\n",
    "\n",
    "    # qa_pairs_json = json.dumps(unique_qa_pairs, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # # выводим вопросы и ответы в разные списки\n",
    "    # questions = [item[\"question\"] for item in unique_qa_pairs]\n",
    "    # answers = [item[\"answer\"] for item in unique_qa_pairs]\n",
    "\n",
    "    # запишем черновой вариант базы в json\n",
    "    write_json(unique_qa_pairs, 'base/output/unique_qa_pairs.json')\n",
    "    return unique_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_round_make_base(unique_qa_pairs:list) -> list:\n",
    "    \"\"\" модуль повторной проверки и очистки вопрос/ответ  \"\"\"\n",
    "\n",
    "    # повторный прогон базы через чат гпт для удаления неккоректных данных\n",
    "    cleaned_unique_qa_pairs = clean_with_chatgpt(unique_qa_pairs)\n",
    "\n",
    "    # print(len(cleaned_unique_qa_pairs))\n",
    "\n",
    "    # Удаление дублирующихся вопросов\n",
    "    double_clean_unique_qa_pairs = remove_duplicate_questions_list(cleaned_unique_qa_pairs)\n",
    "\n",
    "    # # перевод в json формат\n",
    "    # double_clean_unique_qa_pairs_json = json.dumps(double_clean_unique_qa_pairs, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # # извлечение вопросов и ответов\n",
    "    # questions = [item[\"question\"] for item in double_clean_unique_qa_pairs]\n",
    "    # answers = [item[\"answer\"] for item in double_clean_unique_qa_pairs]\n",
    "\n",
    "    # сохраним в файл json итоговый вариант очищенной базы \n",
    "    write_json(double_clean_unique_qa_pairs, 'base/output/double_clean_unique_qa_pairs.json')\n",
    "    len(double_clean_unique_qa_pairs)\n",
    "    return double_clean_unique_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_round_make_base(double_clean_unique_qa_pairs:list) -> list:\n",
    "    \"\"\" третий прогон через chatGPT: проверка и очистка вопрос/ответ \"\"\"\n",
    "    \n",
    "    # загрузка из два раза обработанного файла\n",
    "    double_clean_unique_qa_pairs = read_json('base/output/double_clean_unique_qa_pairs.json')\n",
    "\n",
    "    # третий раунд очистки через GPT\n",
    "    triple_clean_unique_qa_pairs = clean_with_chatgpt_else(double_clean_unique_qa_pairs)\n",
    "\n",
    "    # сохраним резултат в json\n",
    "    write_json(triple_clean_unique_qa_pairs, 'base/output/triple_clean_unique_qa_pairs.json')\n",
    "    # print(len(triple_clean_unique_qa_pairs))\n",
    "    # triple_clean_unique_qa_pairs\n",
    "\n",
    "    \n",
    "    return triple_clean_unique_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_make(triple_clean_unique_qa_pairs:list):\n",
    "    ''' определения перечня категорий из итоговых вариантов вопрос/ответ'''\n",
    "\n",
    "    # Определение категорий\n",
    "    questions = [item[\"question\"] for item in triple_clean_unique_qa_pairs]\n",
    "    categories = determine_categories(questions[:150]) # ограничение по токенам для модели gpt-3.5-turbo-0125 16000 токенов. Поэтому выводим не более 150 вопросов\n",
    "\n",
    "    # запишем в файл категории\n",
    "    with open('base/output/categories.txt', 'w', encoding='UTF8') as f:\n",
    "        f.write(categories)\n",
    "\n",
    "    # print(\"Определенные категории и подкатегории:\")\n",
    "    # print(categories)\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_base(triple_clean_unique_qa_pairs, categories):\n",
    "    ''' модуль создания базы знаний из троекратно обработанного файла почты '''\n",
    "\n",
    "    # # загрузим категории из файла base/output/categories.txt\n",
    "    # with open('base/output/categories.txt', 'r', encoding='UTF8') as f:\n",
    "    #     categories = f.read()\n",
    "    # print(categories)\n",
    "\n",
    "    # создадим json для корректной работы create_knowledge_base\n",
    "    triple_clean_unique_qa_pairs_json = json.dumps(triple_clean_unique_qa_pairs, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Создание базы знаний с помощью chatGPT\n",
    "    knowledge_base = create_knowledge_base(triple_clean_unique_qa_pairs_json, categories)\n",
    "\n",
    "    # Сохраненим в файл\n",
    "    with open('base/output/base_final.md', 'w', encoding='utf-8') as file:\n",
    "        file.write(knowledge_base)\n",
    "\n",
    "    # сохраним в excel    \n",
    "    df = markdown_to_dataframe('base/output/base_final.md')\n",
    "    # # сохраним в excel для проверки\n",
    "    df.to_excel('base/output/base.xlsx')   \n",
    "\n",
    "    print(\"Создание базы знаний завершено.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Очистка данных завершена.\n",
      "Всего символов в файле:1,463,057\n",
      "\n",
      "исключен  результат process_segment\n",
      "Спасибо за ваш запрос. Пожалуйста, предоставьте текст с вопросом и ответом для извлечения и обработки.\n",
      "\n",
      "\n",
      "исключен  результат process_segment\n",
      "Спасибо за предоставленную информацию. Пожалуйста, дайте мне немного времени, чтобы извлечь и обработать вопрос и ответ из сообщения.\n",
      "\n",
      "После первого прогона полчили пар вопрос/ответ:  42\n",
      "После второго прогона полчили пар вопрос/ответ:  18\n",
      "После третьего прогона полчили пар вопрос/ответ:  4\n",
      "получили категорий:  482\n",
      "Создание базы знаний завершено.\n"
     ]
    }
   ],
   "source": [
    "''' главный модуль программы '''\n",
    "# проверим наличие и при необходимости создадим требуемые каталоги\n",
    "make_dirs()\n",
    "\n",
    "base_dir = 'base/'                                    # зашито в make_dirs\n",
    "filename = base_dir + 'input/emails.json'             # путь к обрабатываемому файлу исходящей почтой\n",
    "output_filename = base_dir + 'output/emails.txt'      # путь к обработанному файлу почты \n",
    "structured_file_dir = base_dir + 'structured'         # каталог к временным структурированным файлам обработанными в chatGPT\n",
    "faiss_db = f'{base_dir}output/faiss_db'               # база FAISS для сохранения\n",
    "base_file = f'{base_dir}output/base.txt'              # итоговый файл базы размеченный в маркдаун\n",
    "\n",
    "\"\"\" модуль парсинга почты из json\"\"\"\n",
    "# расскоментить если начинаем парсить почту из json\n",
    "import email_parser\n",
    "# парсим input/emails.json и сохраняем в output/emails.txt\n",
    "email_parser.main(filename, output_filename)\n",
    "\"\"\" -------------------------------\"\"\"\n",
    "\n",
    "# Загрузка почты  из файла txt\n",
    "with open(output_filename, 'r', encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Очистка данных\n",
    "cleaned_data = remove_personal_data(data)\n",
    "\n",
    "# Сохранение очищенных данных в новый файл\n",
    "with open(f'{base_dir}/output/cleaned_emails.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_data)\n",
    "\n",
    "print(\"Очистка данных завершена.\")\n",
    "print(f\"Всего символов в файле:{len(cleaned_data):,}\")\n",
    "\n",
    "\"\"\" создание базы \"\"\"\n",
    "unique_qa_pairs = first_round_make_base(cleaned_data[:50000])    # производим первый прогон частично обработанных писем через GPT   \n",
    "print('После первого прогона получили пар вопрос/ответ: ', len(unique_qa_pairs))   \n",
    "double_clean_unique_qa_pairs = second_round_make_base(unique_qa_pairs) # производим второй прогон через GPT извлеченных пар вопрос/ответ\n",
    "print('После второго прогона получили пар вопрос/ответ: ', len(double_clean_unique_qa_pairs))\n",
    "triple_clean_unique_qa_pairs = triple_round_make_base(double_clean_unique_qa_pairs) # производим третий прогон через GPT извлеченных пар вопрос/ответ\n",
    "print('После третьего прогона получили пар вопрос/ответ: ', len(triple_clean_unique_qa_pairs))\n",
    "categories = categories_make(triple_clean_unique_qa_pairs)  # определяем категории полученных вопросов\n",
    "print('получили категорий и подкатегорий: ', len(categories.replace('\\n\\n','\\n').split('\\n')))\n",
    "\n",
    "# загрузим категории из файла base/output/categories.txt. После создания категорий можно проверить и скорректировать вручную\n",
    "with open('base/output/categories.txt', 'r', encoding='UTF8') as f:\n",
    "    categories = f.read()\n",
    "print(categories)\n",
    "\n",
    "make_base(triple_clean_unique_qa_pairs, categories) # создаем базу. сохраняем сразу в md и excel\n",
    "\n",
    "# конвертим ее в docx для заливки на google docs\n",
    "mdTOdocx.convert_md_to_docx('base/output/base_final.md', 'base/output/base_final.docx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
